import streamlit as st
import openai
import os
import tempfile
from streamlit_webrtc import webrtc_streamer, WebRtcMode
import av
import queue
import time

# üîë Ustaw klucz z Render ENV vars
openai.api_key = os.getenv("OPENAI_API_KEY")

# üîä Tytu≈Ç aplikacji
st.title("üéôÔ∏è AI Asystent Sprzeda≈ºy B2B")
st.write("Nagraj rozmowƒô w przeglƒÖdarce. Asystent stworzy pytania, coaching i follow-up.")

# Kolejka i procesor audio
audio_buffer = queue.Queue()

class AudioProcessor:
    def __init__(self):
        self.recording = False
        self.audio_frames = []

    def recv(self, frame: av.AudioFrame):
        if self.recording:
            self.audio_frames.append(frame)
        return frame

processor = AudioProcessor()

# WebRTC
webrtc_ctx = webrtc_streamer(
    key="audio",
    mode=WebRtcMode.SENDONLY,
    audio_frame_callback=processor.recv,
    media_stream_constraints={"audio": True, "video": False}
)

# ‚è∫Ô∏è Przyciski nagrywania
if webrtc_ctx.state.playing:
    if st.button("‚è∫Ô∏è Rozpocznij nagrywanie"):
        processor.recording = True
        processor.audio_frames = []
        st.session_state["recording_started"] = time.time()

    if st.button("‚èπÔ∏è Zatrzymaj nagrywanie i prze≈õlij"):
        processor.recording = False

        if processor.audio_frames:
            with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as f:
                container = av.open(f.name, mode='w', format='wav')
                stream = container.add_stream("pcm_s16le", rate=processor.audio_frames[0].sample_rate)
                stream.channels = processor.audio_frames[0].layout.channels

                for frame in processor.audio_frames:
                    frame.pts = None
                    container.mux(stream.encode(frame))
                container.mux(stream.encode())
                container.close()
                audio_path = f.name

            # üì§ Prze≈õlij audio do OpenAI Whisper
            with st.spinner("‚è≥ Przesy≈Çam do AI..."):
                with open(audio_path, "rb") as audio_file:
                    transcript_response = openai.audio.transcriptions.create(
                        model="whisper-1",
                        file=audio_file
                    )
                    transcript = transcript_response.text

            # üß† Prompt do GPT
            prompt = f"""
Jeste≈õ AI-asystentem handlowca. Analizujesz rozmowƒô z klientem.

Na podstawie rozmowy:
1. Podaj 2‚Äì3 pytania, kt√≥re warto zadaƒá klientowi.
2. Zasugeruj 1 konkretnƒÖ zmianƒô w zachowaniu handlowca (coaching).
3. Zasugeruj dzia≈Çanie follow-up.

Rozmowa:
{transcript}
"""

            completion = openai.chat.completions.create(
                model="gpt-4",
                messages=[{"role": "user", "content": prompt}]
            )

            result = completion.choices[0].message.content

            # ‚úÖ Wyniki
            st.success("Gotowe!")
            st.markdown("### üí° Sugestie AI")
            st.write(result)

        else:
            st.warning("Brak nagranego d≈∫wiƒôku.")
